import warnings
warnings.filterwarnings('ignore')

import pandas as pd
from pandas_profiling import ProfileReport  
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

print(f' Libraries successfully imported')

dataset = pd.read_csv('Microsoft_malware_dataset_min.csv', nrows = 5000)
dataset

#Perform exploratory data analysis
# Display first five(5) rows
dataset.head()

#Dispay dataset info
dataset.info()

#Statistical analysis on dataset
dataset.describe().T

#Check for missing values
dataset.isnull().sum()

profile = ProfileReport(dataset)
profile.to_file("Microsoft_Malware_profile_report.html")

# Fill numeric columns with median and categorical columns with mode
for col in dataset.select_dtypes(include=["float", "int"]).columns:
    dataset[col].fillna(dataset[col].median(), inplace=True)

for col in dataset.select_dtypes(include=["object"]).columns:
    dataset[col].fillna(dataset[col].mode()[0], inplace=True)

# Check for duplicates 
duplicates = dataset[dataset.duplicated()]

# Display duplicates, if any
if not duplicates.empty:
    print(f"Number of duplicate rows: {duplicates.shape[0]}")
    print(duplicates)
else:
    print("No duplicates found.")

# Remove Duplicates
dataset.drop_duplicates(inplace=True)

duplicates = dataset[dataset.duplicated()]

# Display duplicates, if any
if not duplicates.empty:
    print(f"Number of duplicate rows: {duplicates.shape[0]}")
    print(duplicates)
else:
    print("No duplicates found.")

# Handle Outliers (Example using IQR)
for col in dataset.select_dtypes(include=["float", "int"]).columns:
    Q1 = dataset[col].quantile(0.25)
    Q3 = dataset[col].quantile(0.75)
    IQR = Q3 - Q1
    dataset = dataset[(dataset[col] >= (Q1 - 1.5 * IQR)) & (dataset[col] <= (Q3 + 1.5 * IQR))]

# Encode Categorical Features
dataset = pd.get_dummies(dataset, drop_first=True)
dataset

# Prepare data for modeling
X = dataset[['Census_IsVirtualDevice', 'Census_HasOpticalDiskDrive', 'Firewall', 'IsProtected']]
y = dataset["HasDetections"]

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Decision Tree Model
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)

# Model Evaluation and ROC Curve
y_pred_proba = dt_model.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = roc_auc_score(y_test, y_pred_proba)

%matplotlib inline
plt.figure()
plt.plot(fpr, tpr, label=f"Decision Tree (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="best")
plt.show()

# Hyperparameter Tuning 
dt_model_tuned = DecisionTreeClassifier(max_depth=5, min_samples_split=10)
dt_model_tuned.fit(X_train, y_train)
y_pred_proba_tuned = dt_model_tuned.predict_proba(X_test)[:, 1]
roc_auc_tuned = roc_auc_score(y_test, y_pred_proba_tuned)
print(f"Tuned Model AUC: {roc_auc_tuned:.2f}")

# Drop the target column
dataset_clustering = dataset.drop("HasDetections", axis=1)  

# Apply K-Means Clustering
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Determine optimal K using the Elbow Method
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(dataset_clustering)
    sse.append(kmeans.inertia_)

%matplotlib inline
plt.figure()
plt.plot(range(1, 11), sse, marker='o')
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.title("Elbow Method to Find Optimal K")
plt.show()

# Using elbow method, k = 2
optimal_k = 2  
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
dataset_clustering["Cluster"] = kmeans.fit_predict(dataset_clustering)

# PCA for Visualization (optional, if dataset is high-dimensional)
pca = PCA(n_components=2)
pca_result = pca.fit_transform(dataset_clustering.drop("Cluster", axis=1))

%matplotlib inline
plt.figure()
sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1], hue=dataset_clustering["Cluster"], palette="viridis")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.title("Clusters Visualization")
plt.show()

#Calculating Silhouette Score to evaluate the model

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

# Scale the data (important for clustering)
scaler = StandardScaler()
data_scaled = scaler.fit_transform(dataset)  

# Fit K-means model
kmeans = KMeans(n_clusters=2, init='k-means++', random_state=42)  
labels = kmeans.fit_predict(data_scaled)

# Calculate Silhouette Score
sil_score = silhouette_score(data_scaled, labels)
print(f'Silhouette Score for K-means clustering with k={2}: {sil_score}')



